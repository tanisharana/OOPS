{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pawan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pawan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "\n",
    "data = pd.read_csv(\"data.csv\",nrows=10000)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "abstracts = df['abstract'].tolist()\n",
    "titles = df['title'].tolist()\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove unnecessary characters and convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize into sentences\n",
    "        sentences = tokenizer.tokenize(text)\n",
    "        # Tokenize into words\n",
    "        words = [word for sentence in sentences for word in nltk.word_tokenize(sentence)]\n",
    "        # Remove stopwords and punctuation\n",
    "        words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        return ' '.join(words), sentences\n",
    "    else:\n",
    "        return '', []\n",
    "\n",
    "\n",
    "processed_abstracts, abstract_sentences = zip(*[preprocess_text(abstract) if abstract else ('', []) for abstract in abstracts])\n",
    "processed_titles, _ = zip(*[preprocess_text(title) for title in titles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant features - TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_abstracts)\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "tfidf_matrix = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# Sentence scoring - TF-IDF scores\n",
    "def calculate_tfidf_scores(matrix, sentences):\n",
    "    tfidf_scores = matrix.toarray()\n",
    "    num_documents = tfidf_scores.shape[0]\n",
    "    for i in range(num_documents):\n",
    "        abstract_sentence_scores = tfidf_scores[i]\n",
    "        sentence_indices = [j for j in range(num_documents) if j != i]  # Exclude current abstract\n",
    "        for j in sentence_indices:\n",
    "            abstract_sentence_scores += tfidf_scores[j]\n",
    "        tfidf_scores[i] = abstract_sentence_scores / (num_documents - 1)  # Divide by number of other sentences\n",
    "    return tfidf_scores.mean(axis=1)\n",
    "\n",
    "tfidf_scores = calculate_tfidf_scores(tfidf_matrix, abstract_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top sentences\n",
    "def select_top_sentences(scores, sentences, n=3):\n",
    "    sorted_sentences = [sentence for _, sentence in sorted(zip(scores, sentences), reverse=True)]\n",
    "    return sorted_sentences[:n]\n",
    "\n",
    "selected_sentences = select_top_sentences(tfidf_scores, abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] Hourly sea level observations measured by five tide gauges at Santa Cruz harbor (Tenerife Island), in the Northeastern Tropical Atlantic, have been merged to build a consistent and almost continuous sea level record starting in 1927.',\n",
       " 'Datum continuity was ensured using high precision leveling information.',\n",
       " 'The time series underwent a detailed quality control in order to remove outliers, time drifts, and datum shifts.',\n",
       " 'The resulting sea level record was then used to describe the low frequency (interannual to decadal) sea level variability at Tenerife.',\n",
       " 'It was found that at interannual and longer time scales, the observed sea level changes are primarily driven by steric sea level variations.',\n",
       " 'Such steric changes are originated by coastal trapped waves induced by longshore winds along the continental coast and propagate poleward.',\n",
       " 'Observed sea level rise at Tenerife was 2.09 6 0.04 mm/yr since 1927.',\n",
       " 'According to the hydrographic observations in the area, only half of this trend was attributed to steric sea level changes for the top 500 m, at least since 1950.',\n",
       " 'This paper focuses on the application of adjustable speed induction motor drives for gantry cranes.',\n",
       " 'Modern solution considers application of frequency converters for all drives.',\n",
       " 'Multi-motor drives are standard solutions in crane application and requirements of load sharing are present.',\n",
       " 'Presented algorithm provides load sharing proportional to the rated motor power on the simple and practically applicable method on the basis of estimated torques by frequency converters, and controller realized in PLC.',\n",
       " 'Special attention is devoted to wide span gantry drive and algorithm for skew elimination.',\n",
       " 'Solutions for load distribution in multi-motor drive, as well as mode of gantry drive skew elimination, are described.',\n",
       " 'Suggested solution concept is confirmed by the experimental results.',\n",
       " 'This thesis studies the representation theory of Cherednik algebras associated to a complex algebraic varieties which carries the action of a finite group.',\n",
       " 'First, we prove that the Knizhnik-Zamolodchikov functor from the category of &-coherent modules for the Cherednik algebra to finite dimensional modules over the Hecke algebra is essentially surjective.',\n",
       " 'Then we begin to use this result to study the analog of category 0 for Cherednik algebras on Riemann surfaces and on products of elliptic curves.',\n",
       " 'In particular we give conditions on the parameters under which this category 0 analog is nonzero.',\n",
       " 'Our next goal is to generalize several basic results from the theory of 9-modules to the representation theory of rational Cherednik algebras.',\n",
       " 'We relate characterizations of holonomic modules in terms of singular support and Gelfand-Kirillov dimension.',\n",
       " 'We study pullback, pushforward, and dual on the derived category of (holonomic) Cherednik modules for certain classes of maps between varieties.',\n",
       " 'We prove, in the case of generic parameters for the rational Cherednik algebra, that pushforward with respect to an open affine inclusion preserves holonomicity.',\n",
       " \"Finally, we relate the global sections algebra of the sheaf of Cherednik algebras associated to a smooth quadric hypersurface of P' to the Dunkl angular momentum algebra of Feigin-Hakobyan.\",\n",
       " 'In particular, this lets us relate the angular momentum algebra for a rank 3 Coxeter group to the rank 2 symplectic reflection algebra for a corresponding finite subgroup of SL 2 .',\n",
       " 'Thesis Supervisor: Pavel Etingof Title: Professor of Mathematics']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summary using LexRank\n",
    "def generate_summary(sentences, n=1000):\n",
    "    parser = PlaintextParser.from_string(' '.join(sentences), Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summarizer.stop_words = get_stop_words(\"english\")\n",
    "    summary = summarizer(document=parser.document, sentences_count=n)\n",
    "    return [str(sentence) for sentence in summary]\n",
    "\n",
    "generated_summary = generate_summary(selected_sentences)\n",
    "generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated summaries:10000\n",
      "Number of processedÂ titles:10000\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from rouge import Rouge\n",
    "\n",
    "# Flatten the lists of summaries and titles\n",
    "generated_summary_flat = list(chain.from_iterable(generated_summary))\n",
    "processed_titles_flat = list(chain.from_iterable(processed_titles))\n",
    "\n",
    "# print(\"Number of generated summaries:\", len(generated_summary_flat))\n",
    "# print(\"Number of processed titles:\", len(processed_titles_flat))\n",
    "print(\"Number of generated summaries:\", len(generated_summary_flat))\n",
    "print(\"Number of processed titles:\", len(processed_titles_flat))\n",
    "# # Evaluate the performance\n",
    "# rouge = Rouge()\n",
    "# scores = rouge.get_scores(generated_summary_flat, processed_titles_flat, avg=True)\n",
    "\n",
    "# print(\"ROUGE Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beLU Measure: 0.3953212504743868\n"
     ]
    }
   ],
   "source": [
    "# Additional evaluation using beLU measure\n",
    "def calculate_belue_measure(generated, ground_truth):\n",
    "    generated_tokens = nltk.word_tokenize(generated)\n",
    "    ground_truth_tokens = nltk.word_tokenize(ground_truth)\n",
    "    \n",
    "    intersection = len(set(generated_tokens) & set(ground_truth_tokens))\n",
    "    recall = intersection / len(ground_truth_tokens)\n",
    "    precision = intersection / len(generated_tokens)\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "belue_scores = [calculate_belue_measure(summary, title) for summary, title in zip(generated_summary, titles)]\n",
    "average_belue_score = (sum(belue_scores) / len(belue_scores))\n",
    "\n",
    "print(\"beLU Measure:\", average_belue_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
